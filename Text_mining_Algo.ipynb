{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_mining_Algo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDS3FJ77wlaX",
        "colab_type": "text"
      },
      "source": [
        "# N-Grams \n",
        "\n",
        "In Python, we can implement N-Gram using NLTK library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L_B0BVgw3tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2368e762-2a66-4c1a-8d2f-4a0685796ce5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYGVYT-wwQbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "text = 'FinTechExplained is a publication'\n",
        "\n",
        "\n",
        "one_grams = ngrams(nltk.word_tokenize(text), 1)\n",
        "two_grams = ngrams(nltk.word_tokenize(text), 2)\n",
        "three_grams = ngrams(nltk.word_tokenize(text), 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo0mb2CZwxSw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b7d46400-527e-436f-f30f-93a66dee4514"
      },
      "source": [
        "for item in one_grams:\n",
        "  print(item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('FinTechExplained',)\n",
            "('is',)\n",
            "('a',)\n",
            "('publication',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5zdLbOxCR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "35edbf2d-fffe-40af-f74d-d967d8c8448b"
      },
      "source": [
        "for item in two_grams:\n",
        "  print(item)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('FinTechExplained', 'is')\n",
            "('is', 'a')\n",
            "('a', 'publication')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERXcp1H4xkpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e002b213-1405-4b74-e106-69aee2cda240"
      },
      "source": [
        "for item in three_grams:\n",
        "  print(item)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('FinTechExplained', 'is', 'a')\n",
            "('is', 'a', 'publication')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCrMQFocyNzi",
        "colab_type": "text"
      },
      "source": [
        "# Bag Of Words\n",
        "\n",
        "###We can achieve it using Sci-kit learn library in Python\n",
        "\n",
        "\n",
        "## get_tweets() is a seperate function used to retrieve the 100 tweets using consumer key \n",
        "\n",
        "## Similarly, get_fb_statuses()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkmWonrRyNe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = {'twitter':get_tweets(),\n",
        "        'facebook':get_fb_statuses()}\n",
        "vectoriser = CountVectorizer()\n",
        "vec = vectoriser.fit_transform(data['twitter'].append(data['facebook']))\n",
        "df = pd.DataFrame(vec.toarray().transpose(), index = vectoriser.get_feature_names())\n",
        "df.columns = ['twitter', 'facebook']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpxaVulP1EdK",
        "colab_type": "text"
      },
      "source": [
        "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "###In NLP projects, we are required to determine the importance of each word. TF-IDF is a great statistical measure. It helps us understand the relevance of the term (word)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBrr4YbP1D7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vectoriser = TfidfVectorizer()\n",
        "vec = vectoriser.fit_transform(data['twitter'].append(data['facebook']))\n",
        "df = pd.DataFrame(vec.toarray().transpose(), index = vectoriser.get_feature_names())\n",
        "df.columns = ['twitter', 'facebook']\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}